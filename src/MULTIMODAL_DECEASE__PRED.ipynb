{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYpRso6GynzQ"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets tensorflow matplotlib scikit-learn seaborn plotly\n",
        "!pip install opencv-python pillow pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NpcCBRNByp5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DISEASE_CONFIG = {\n",
        "    'pneumonia': {\n",
        "        'dataset_url': 'https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia',\n",
        "        'classes': ['NORMAL', 'PNEUMONIA'],\n",
        "        'img_size': (224, 224),\n",
        "        'class_mode': 'binary',\n",
        "        'loss': 'binary_crossentropy',\n",
        "        'metrics': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
        "        'base_model': DenseNet121\n",
        "    },\n",
        "    'skin_cancer': {\n",
        "        'dataset_url': 'https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign',\n",
        "        'classes': ['benign', 'malignant'],\n",
        "        'img_size': (224, 224),\n",
        "        'class_mode': 'binary',\n",
        "        'loss': 'binary_crossentropy',\n",
        "        'metrics': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
        "        'base_model': EfficientNetB0\n",
        "    },\n",
        "    'brain_tumor': {\n",
        "        'dataset_url': 'https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection',\n",
        "        'classes': ['no', 'yes'],\n",
        "        'img_size': (224, 224),\n",
        "        'class_mode': 'binary',\n",
        "        'loss': 'binary_crossentropy',\n",
        "        'metrics': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
        "        'base_model': ResNet50\n",
        "    },\n",
        "    'covid19': {\n",
        "        'dataset_url': 'https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database',\n",
        "        'classes': ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia'],\n",
        "        'img_size': (224, 224),\n",
        "        'class_mode': 'categorical',\n",
        "        'loss': 'categorical_crossentropy',\n",
        "        'metrics': ['accuracy'],\n",
        "        'base_model': DenseNet121\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "fUyvGz8nytmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiDiseaseDatasetManager:\n",
        "    def __init__(self):\n",
        "        self.setup_kaggle()\n",
        "\n",
        "    def setup_kaggle(self):\n",
        "        \"\"\"Setup Kaggle API credentials\"\"\"\n",
        "        if not Path('kaggle.json').exists():\n",
        "            print(\"Please upload your kaggle.json file.\")\n",
        "            from google.colab import files\n",
        "            files.upload()\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "        !chmod 600 /content/kaggle.json\n",
        "\n",
        "    def download_dataset(self, disease_name, dataset_url):\n",
        "        \"\"\"Download and organize dataset for specific disease\"\"\"\n",
        "        print(f\"Downloading {disease_name} dataset...\")\n",
        "        import opendatasets as od\n",
        "\n",
        "        data_dir = f\"data/{disease_name}\"\n",
        "        od.download(dataset_url, data_dir=data_dir)\n",
        "\n",
        "        # Organize dataset structure based on disease type\n",
        "        self.organize_dataset(disease_name, data_dir)\n",
        "\n",
        "    def organize_dataset(self, disease_name, data_dir):\n",
        "        \"\"\"Organize dataset structure\"\"\"\n",
        "        if disease_name == 'pneumonia':\n",
        "            # Handle chest X-ray pneumonia dataset\n",
        "            if Path(f\"{data_dir}/chest-xray-pneumonia\").exists():\n",
        "                shutil.move(f\"{data_dir}/chest-xray-pneumonia/chest_xray\", f\"{data_dir}/chest_xray\")\n",
        "                shutil.rmtree(f\"{data_dir}/chest-xray-pneumonia\")\n",
        "\n",
        "        elif disease_name == 'skin_cancer':\n",
        "            # Handle skin cancer dataset - usually has different structure\n",
        "            self._organize_skin_cancer_dataset(data_dir)\n",
        "\n",
        "        elif disease_name == 'brain_tumor':\n",
        "            # Handle brain tumor dataset\n",
        "            self._organize_brain_tumor_dataset(data_dir)\n",
        "\n",
        "        elif disease_name == 'covid19':\n",
        "            # Handle COVID-19 dataset\n",
        "            self._organize_covid19_dataset(data_dir)\n",
        "\n",
        "        print(f\"Dataset {disease_name} organized successfully!\")\n",
        "\n",
        "    def _organize_skin_cancer_dataset(self, data_dir):\n",
        "        \"\"\"Organize skin cancer dataset structure\"\"\"\n",
        "        # Find the actual dataset folder\n",
        "        possible_paths = [\n",
        "            f\"{data_dir}/skin-cancer-malignant-vs-benign\",\n",
        "            f\"{data_dir}/data\",\n",
        "            f\"{data_dir}/train\",\n",
        "            f\"{data_dir}\"\n",
        "        ]\n",
        "\n",
        "        actual_data_path = None\n",
        "        for path in possible_paths:\n",
        "            if Path(path).exists():\n",
        "                # Check if it contains class folders\n",
        "                subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "                if any(class_name in subdirs for class_name in ['benign', 'malignant', 'train', 'test']):\n",
        "                    actual_data_path = path\n",
        "                    break\n",
        "\n",
        "        if actual_data_path:\n",
        "            # If data is directly in class folders, create train/val/test split\n",
        "            if 'benign' in os.listdir(actual_data_path) and 'malignant' in os.listdir(actual_data_path):\n",
        "                self._create_train_val_test_split(actual_data_path, ['benign', 'malignant'])\n",
        "\n",
        "    def _organize_brain_tumor_dataset(self, data_dir):\n",
        "        \"\"\"Organize brain tumor dataset structure\"\"\"\n",
        "        possible_paths = [\n",
        "            f\"{data_dir}/brain-tumor-detection\",\n",
        "            f\"{data_dir}/data\",\n",
        "            f\"{data_dir}\"\n",
        "        ]\n",
        "\n",
        "        actual_data_path = None\n",
        "        for path in possible_paths:\n",
        "            if Path(path).exists():\n",
        "                subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "                if any(class_name in subdirs for class_name in ['no', 'yes', 'train', 'test']):\n",
        "                    actual_data_path = path\n",
        "                    break\n",
        "\n",
        "        if actual_data_path and 'no' in os.listdir(actual_data_path):\n",
        "            self._create_train_val_test_split(actual_data_path, ['no', 'yes'])\n",
        "\n",
        "    def _organize_covid19_dataset(self, data_dir):\n",
        "        \"\"\"Organize COVID-19 dataset structure\"\"\"\n",
        "        possible_paths = [\n",
        "            f\"{data_dir}/covid19-radiography-database\",\n",
        "            f\"{data_dir}/COVID-19_Radiography_Dataset\",\n",
        "            f\"{data_dir}\"\n",
        "        ]\n",
        "\n",
        "        actual_data_path = None\n",
        "        for path in possible_paths:\n",
        "            if Path(path).exists():\n",
        "                subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "                covid_classes = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
        "                if any(class_name in subdirs for class_name in covid_classes):\n",
        "                    actual_data_path = path\n",
        "                    break\n",
        "\n",
        "        if actual_data_path:\n",
        "            self._create_train_val_test_split(actual_data_path, ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia'])\n",
        "\n",
        "    def _create_train_val_test_split(self, data_path, class_names, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "        \"\"\"Create train/validation/test split from class folders\"\"\"\n",
        "        import random\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        random.seed(42)\n",
        "\n",
        "        base_dir = os.path.dirname(data_path)\n",
        "        train_dir = os.path.join(base_dir, 'train')\n",
        "        val_dir = os.path.join(base_dir, 'val')\n",
        "        test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "        # Create train, val, and test directories\n",
        "        os.makedirs(train_dir, exist_ok=True)\n",
        "        os.makedirs(val_dir, exist_ok=True)\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Creating train/val/test split with ratios: {train_ratio:.1%}/{val_ratio:.1%}/{test_ratio:.1%}\")\n",
        "\n",
        "        total_files = 0\n",
        "        for class_name in class_names:\n",
        "            class_path = os.path.join(data_path, class_name)\n",
        "            if not os.path.exists(class_path):\n",
        "                print(f\"Warning: Class folder {class_name} not found, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Create class subdirectories\n",
        "            os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "            # Get all image files with various extensions\n",
        "            image_files = []\n",
        "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif', '*.gif', '*.webp']\n",
        "\n",
        "            for ext in image_extensions:\n",
        "                image_files.extend(glob.glob(os.path.join(class_path, ext)))\n",
        "                image_files.extend(glob.glob(os.path.join(class_path, ext.upper())))\n",
        "\n",
        "            if len(image_files) == 0:\n",
        "                print(f\"Warning: No image files found in {class_path}\")\n",
        "                continue\n",
        "\n",
        "            # Shuffle files for random distribution\n",
        "            random.shuffle(image_files)\n",
        "\n",
        "            # Calculate split indices\n",
        "            total_images = len(image_files)\n",
        "            train_end = int(total_images * train_ratio)\n",
        "            val_end = train_end + int(total_images * val_ratio)\n",
        "\n",
        "            # Split files\n",
        "            train_files = image_files[:train_end]\n",
        "            val_files = image_files[train_end:val_end]\n",
        "            test_files = image_files[val_end:]\n",
        "\n",
        "            # Function to copy files safely\n",
        "            def copy_files_safely(file_list, dest_dir, split_name):\n",
        "                copied_count = 0\n",
        "                for file_path in file_list:\n",
        "                    try:\n",
        "                        filename = os.path.basename(file_path)\n",
        "                        dest_path = os.path.join(dest_dir, class_name, filename)\n",
        "                        if not os.path.exists(dest_path):\n",
        "                            shutil.copy2(file_path, dest_path)\n",
        "                            copied_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error copying {file_path}: {str(e)}\")\n",
        "                return copied_count\n",
        "\n",
        "            # Copy files to respective directories\n",
        "            train_copied = copy_files_safely(train_files, train_dir, 'train')\n",
        "            val_copied = copy_files_safely(val_files, val_dir, 'val')\n",
        "            test_copied = copy_files_safely(test_files, test_dir, 'test')\n",
        "\n",
        "            total_files += total_images\n",
        "            print(f\"  {class_name}: {train_copied} train, {val_copied} val, {test_copied} test images\")\n",
        "\n",
        "        print(f\"Total images processed: {total_files}\")\n",
        "        return train_dir, val_dir, test_dir\n"
      ],
      "metadata": {
        "id": "s0cCyfLcy2SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 5. Data Generators\n",
        "# ==============================\n",
        "class DataGeneratorFactory:\n",
        "    @staticmethod\n",
        "    def create_generators(disease_name, base_dir, config):\n",
        "        \"\"\"Create data generators for specific disease\"\"\"\n",
        "        img_size = config['img_size']\n",
        "        batch_size = 32\n",
        "\n",
        "        # Data augmentation for training\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "            validation_split=0.2  # Use 20% for validation\n",
        "        )\n",
        "\n",
        "        # No augmentation for validation/test\n",
        "        val_test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "        # Create generators based on dataset structure\n",
        "        if disease_name == 'pneumonia':\n",
        "            train_dir = os.path.join(base_dir, \"chest_xray/train\")\n",
        "            val_dir = os.path.join(base_dir, \"chest_xray/val\")\n",
        "            test_dir = os.path.join(base_dir, \"chest_xray/test\")\n",
        "\n",
        "            if all(os.path.exists(d) for d in [train_dir, val_dir, test_dir]):\n",
        "                train_gen = train_datagen.flow_from_directory(\n",
        "                    train_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode']\n",
        "                )\n",
        "\n",
        "                val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "                    val_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode']\n",
        "                )\n",
        "\n",
        "                test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "                    test_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode'], shuffle=False\n",
        "                )\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"Pneumonia dataset structure not found in {base_dir}\")\n",
        "\n",
        "        else:\n",
        "            # For other datasets with created train/val/test split\n",
        "            train_dir = os.path.join(base_dir, \"train\")\n",
        "            val_dir = os.path.join(base_dir, \"val\")\n",
        "            test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "            # Check if train/val/test directories exist\n",
        "            if all(os.path.exists(d) for d in [train_dir, val_dir, test_dir]):\n",
        "                train_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "                    train_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode']\n",
        "                )\n",
        "\n",
        "                val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "                    val_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode']\n",
        "                )\n",
        "\n",
        "                test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "                    test_dir, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode'], shuffle=False\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                # Fallback: use validation_split on single directory if splits don't exist\n",
        "                possible_dirs = [\n",
        "                    os.path.join(base_dir, \"train\"),\n",
        "                    base_dir\n",
        "                ]\n",
        "\n",
        "                data_directory = None\n",
        "                for dir_path in possible_dirs:\n",
        "                    if os.path.exists(dir_path):\n",
        "                        try:\n",
        "                            subdirs = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]\n",
        "                            if len(subdirs) >= 2:  # Has class folders\n",
        "                                data_directory = dir_path\n",
        "                                break\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                if data_directory is None:\n",
        "                    raise FileNotFoundError(f\"Could not find organized data directory for {disease_name}\")\n",
        "\n",
        "                print(f\"Using fallback validation_split for {disease_name}\")\n",
        "                train_gen = train_datagen.flow_from_directory(\n",
        "                    data_directory, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode'], subset='training'\n",
        "                )\n",
        "\n",
        "                val_gen = val_test_datagen.flow_from_directory(\n",
        "                    data_directory, target_size=img_size, batch_size=batch_size,\n",
        "                    class_mode=config['class_mode'], subset='validation'\n",
        "                )\n",
        "\n",
        "                # Use validation set as test set\n",
        "                test_gen = val_gen\n",
        "\n",
        "        return train_gen, val_gen, test_gen\n"
      ],
      "metadata": {
        "id": "eX8NJte1y3KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiDiseaseModelBuilder:\n",
        "    @staticmethod\n",
        "    def build_model(disease_name, config):\n",
        "        \"\"\"Build model for specific disease\"\"\"\n",
        "        input_shape = config['img_size'] + (3,)\n",
        "        base_model_class = config['base_model']\n",
        "\n",
        "        # Load pre-trained base model\n",
        "        base_model = base_model_class(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Determine output neurons based on problem type\n",
        "        if config['class_mode'] == 'binary':\n",
        "            output_neurons = 1\n",
        "            activation = 'sigmoid'\n",
        "        else:\n",
        "            output_neurons = len(config['classes'])\n",
        "            activation = 'softmax'\n",
        "\n",
        "        # Build model\n",
        "        model = models.Sequential([\n",
        "            base_model,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(output_neurons, activation=activation)\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss=config['loss'],\n",
        "            metrics=config['metrics']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "# ==============================\n",
        "# 7. Training Pipeline\n",
        "# ==============================\n",
        "class DiseaseTrainer:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.histories = {}\n",
        "\n",
        "    def train_disease_model(self, disease_name, train_gen, val_gen, config, epochs=20):\n",
        "        \"\"\"Train model for specific disease\"\"\"\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training {disease_name.upper()} Detection Model\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Build model\n",
        "        model = MultiDiseaseModelBuilder.build_model(disease_name, config)\n",
        "        print(f\"Model architecture for {disease_name}:\")\n",
        "        model.summary()\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                f\"best_{disease_name}_model.h5\",\n",
        "                monitor='val_loss',\n",
        "                mode='min',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=7,\n",
        "                mode='min',\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=3,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            steps_per_epoch=max(1, train_gen.samples // train_gen.batch_size),\n",
        "            validation_data=val_gen,\n",
        "            validation_steps=max(1, val_gen.samples // val_gen.batch_size),\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Load best model\n",
        "        model = tf.keras.models.load_model(f\"best_{disease_name}_model.h5\")\n",
        "\n",
        "        # Store model and history\n",
        "        self.models[disease_name] = model\n",
        "        self.histories[disease_name] = history\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    def train_all_diseases(self, diseases_to_train, epochs=20):\n",
        "        \"\"\"Train models for all specified diseases with error handling\"\"\"\n",
        "        dataset_manager = MultiDiseaseDatasetManager()\n",
        "\n",
        "        successful_trainings = []\n",
        "        failed_trainings = []\n",
        "\n",
        "        for disease_name in diseases_to_train:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Processing {disease_name.upper()}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            if disease_name not in DISEASE_CONFIG:\n",
        "                print(f\"Warning: {disease_name} not found in config. Skipping...\")\n",
        "                failed_trainings.append((disease_name, \"Not in config\"))\n",
        "                continue\n",
        "\n",
        "            # Check if model already exists\n",
        "            model_path = f\"best_{disease_name}_model.h5\"\n",
        "            if os.path.exists(model_path):\n",
        "                print(f\"Model for {disease_name} already exists! Loading existing model...\")\n",
        "                try:\n",
        "                    model = tf.keras.models.load_model(model_path)\n",
        "                    self.models[disease_name] = model\n",
        "                    successful_trainings.append(disease_name)\n",
        "                    print(f\"✅ Successfully loaded existing {disease_name} model!\")\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load existing model: {e}\")\n",
        "                    print(\"Will retrain the model...\")\n",
        "\n",
        "            try:\n",
        "                config = DISEASE_CONFIG[disease_name]\n",
        "\n",
        "                # Download dataset\n",
        "                print(f\"📥 Downloading {disease_name} dataset...\")\n",
        "                dataset_manager.download_dataset(disease_name, config['dataset_url'])\n",
        "\n",
        "                # Create data generators\n",
        "                print(f\"🔄 Creating data generators for {disease_name}...\")\n",
        "                base_dir = f\"data/{disease_name}\"\n",
        "                train_gen, val_gen, test_gen = DataGeneratorFactory.create_generators(\n",
        "                    disease_name, base_dir, config\n",
        "                )\n",
        "\n",
        "                # Train model\n",
        "                print(f\"🚀 Training {disease_name} model...\")\n",
        "                model, history = self.train_disease_model(\n",
        "                    disease_name, train_gen, val_gen, config, epochs\n",
        "                )\n",
        "\n",
        "                # Evaluate model\n",
        "                print(f\"📊 Evaluating {disease_name} model...\")\n",
        "                self.evaluate_model(disease_name, model, test_gen, config)\n",
        "\n",
        "                successful_trainings.append(disease_name)\n",
        "                print(f\"✅ Successfully completed training for {disease_name}!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error training {disease_name}: {str(e)}\")\n",
        "                failed_trainings.append((disease_name, str(e)))\n",
        "                print(f\"Skipping {disease_name} and continuing with next disease...\")\n",
        "                continue\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"TRAINING SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"✅ Successfully trained: {len(successful_trainings)} models\")\n",
        "        for disease in successful_trainings:\n",
        "            print(f\"   - {disease}\")\n",
        "\n",
        "        if failed_trainings:\n",
        "            print(f\"\\n❌ Failed to train: {len(failed_trainings)} models\")\n",
        "            for disease, error in failed_trainings:\n",
        "                print(f\"   - {disease}: {error}\")\n",
        "\n",
        "        if successful_trainings:\n",
        "            print(f\"\\n🎉 Training completed with {len(successful_trainings)} successful models!\")\n",
        "        else:\n",
        "            print(f\"\\n⚠️  No models were successfully trained.\")\n",
        "\n",
        "        return successful_trainings, failed_trainings\n",
        "\n",
        "    def evaluate_model(self, disease_name, model, test_gen, config):\n",
        "        \"\"\"Evaluate trained model with better error handling\"\"\"\n",
        "        print(f\"\\n{'='*30}\")\n",
        "        print(f\"Evaluating {disease_name.upper()} Model\")\n",
        "        print(f\"{'='*30}\")\n",
        "\n",
        "        try:\n",
        "            # Get predictions\n",
        "            print(\"Generating predictions...\")\n",
        "            predictions = model.predict(test_gen, verbose=1)\n",
        "\n",
        "            # Handle different prediction formats\n",
        "            if config['class_mode'] == 'binary':\n",
        "                if len(predictions.shape) == 2 and predictions.shape[1] == 1:\n",
        "                    # Binary classification with shape (n, 1)\n",
        "                    y_pred = (predictions[:, 0] > 0.5).astype(int)\n",
        "                elif len(predictions.shape) == 1:\n",
        "                    # Binary classification with shape (n,)\n",
        "                    y_pred = (predictions > 0.5).astype(int)\n",
        "                else:\n",
        "                    print(f\"Unexpected prediction shape: {predictions.shape}\")\n",
        "                    y_pred = (predictions.flatten() > 0.5).astype(int)\n",
        "            else:\n",
        "                # Multi-class classification\n",
        "                y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Get true labels\n",
        "            y_true = test_gen.classes\n",
        "\n",
        "            # Ensure arrays have same length\n",
        "            min_length = min(len(y_true), len(y_pred))\n",
        "            y_true = y_true[:min_length]\n",
        "            y_pred = y_pred[:min_length]\n",
        "\n",
        "            print(f\"Evaluation samples: {min_length}\")\n",
        "            print(f\"True labels shape: {y_true.shape}\")\n",
        "            print(f\"Predicted labels shape: {y_pred.shape}\")\n",
        "\n",
        "            # Calculate metrics\n",
        "            test_results = model.evaluate(test_gen, verbose=0)\n",
        "            print(f\"Test Loss: {test_results[0]:.4f}\")\n",
        "            print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
        "\n",
        "            # Classification report\n",
        "            class_names = list(test_gen.class_indices.keys())\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "            # Confusion Matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=class_names, yticklabels=class_names)\n",
        "            plt.title(f'{disease_name.title()} - Confusion Matrix')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during evaluation: {e}\")\n",
        "            print(\"Attempting basic evaluation...\")\n",
        "            try:\n",
        "                test_results = model.evaluate(test_gen, verbose=0)\n",
        "                print(f\"Test Loss: {test_results[0]:.4f}\")\n",
        "                print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Basic evaluation also failed: {e2}\")\n"
      ],
      "metadata": {
        "id": "QqJuG-mKzCQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizationTools:\n",
        "    @staticmethod\n",
        "    def plot_training_history(histories):\n",
        "        \"\"\"Plot training history for all diseases\"\"\"\n",
        "        fig, axes = plt.subplots(2, len(histories), figsize=(20, 10))\n",
        "\n",
        "        for idx, (disease_name, history) in enumerate(histories.items()):\n",
        "            # Plot accuracy\n",
        "            axes[0, idx].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            axes[0, idx].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            axes[0, idx].set_title(f'{disease_name.title()} - Accuracy')\n",
        "            axes[0, idx].set_xlabel('Epoch')\n",
        "            axes[0, idx].set_ylabel('Accuracy')\n",
        "            axes[0, idx].legend()\n",
        "\n",
        "            # Plot loss\n",
        "            axes[1, idx].plot(history.history['loss'], label='Training Loss')\n",
        "            axes[1, idx].plot(history.history['val_loss'], label='Validation Loss')\n",
        "            axes[1, idx].set_title(f'{disease_name.title()} - Loss')\n",
        "            axes[1, idx].set_xlabel('Epoch')\n",
        "            axes[1, idx].set_ylabel('Loss')\n",
        "            axes[1, idx].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_sample_images(generators_dict):\n",
        "        \"\"\"Plot sample images from all datasets\"\"\"\n",
        "        fig, axes = plt.subplots(len(generators_dict), 3, figsize=(15, 5*len(generators_dict)))\n",
        "\n",
        "        for idx, (disease_name, (train_gen, _, _)) in enumerate(generators_dict.items()):\n",
        "            images, labels = next(train_gen)\n",
        "\n",
        "            for i in range(3):\n",
        "                ax = axes[idx, i] if len(generators_dict) > 1 else axes[i]\n",
        "                ax.imshow(images[i])\n",
        "\n",
        "                # Get class name\n",
        "                class_names = list(train_gen.class_indices.keys())\n",
        "                if hasattr(labels[i], '__len__'):  # categorical\n",
        "                    label_idx = np.argmax(labels[i])\n",
        "                else:  # binary\n",
        "                    label_idx = int(labels[i])\n",
        "\n",
        "                ax.set_title(f'{disease_name.title()}: {class_names[label_idx]}')\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "xWCundD3zGBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAMVisualizer:\n",
        "    @staticmethod\n",
        "    def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
        "        \"\"\"Generate Grad-CAM heatmap\"\"\"\n",
        "        # Find the last convolutional layer\n",
        "        for layer in reversed(model.layers):\n",
        "            if 'conv' in layer.name.lower():\n",
        "                last_conv_layer_name = layer.name\n",
        "                break\n",
        "\n",
        "        grad_model = tf.keras.models.Model(\n",
        "            [model.inputs],\n",
        "            [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            conv_outputs, predictions = grad_model(img_array)\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "            class_channel = predictions[:, pred_index]\n",
        "\n",
        "        grads = tape.gradient(class_channel, conv_outputs)\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "        conv_outputs = conv_outputs[0]\n",
        "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "        heatmap = tf.squeeze(heatmap)\n",
        "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "\n",
        "        return heatmap.numpy()\n",
        "\n",
        "    @staticmethod\n",
        "    def display_gradcam(img_path, model, img_size):\n",
        "        \"\"\"Display Grad-CAM visualization\"\"\"\n",
        "        # Load and preprocess image\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img_resized = cv2.resize(img, img_size)\n",
        "        img_array = np.expand_dims(img_resized / 255.0, axis=0)\n",
        "\n",
        "        # Generate heatmap\n",
        "        heatmap = GradCAMVisualizer.make_gradcam_heatmap(img_array, model, None)\n",
        "\n",
        "        # Resize heatmap to original image size\n",
        "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "        # Create overlay\n",
        "        superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "        # Display results\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(heatmap)\n",
        "        plt.title(\"Grad-CAM Heatmap\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.title(\"Overlay\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "D694fMgYzKTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiDiseasePredictor:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.configs = {}\n",
        "\n",
        "    def load_models(self, disease_names):\n",
        "        \"\"\"Load trained models\"\"\"\n",
        "        for disease_name in disease_names:\n",
        "            try:\n",
        "                model_path = f\"best_{disease_name}_model.h5\"\n",
        "                self.models[disease_name] = tf.keras.models.load_model(model_path)\n",
        "                self.configs[disease_name] = DISEASE_CONFIG[disease_name]\n",
        "                print(f\"Loaded {disease_name} model successfully!\")\n",
        "            except:\n",
        "                print(f\"Could not load {disease_name} model. Make sure it's trained first.\")\n",
        "\n",
        "    def predict_image(self, img_path, disease_name):\n",
        "        \"\"\"Predict disease for single image\"\"\"\n",
        "        if disease_name not in self.models:\n",
        "            print(f\"Model for {disease_name} not loaded!\")\n",
        "            return None\n",
        "\n",
        "        model = self.models[disease_name]\n",
        "        config = self.configs[disease_name]\n",
        "        img_size = config['img_size']\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img_array = np.expand_dims(img / 255.0, axis=0)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(img_array)\n",
        "\n",
        "        # Interpret prediction\n",
        "        if config['class_mode'] == 'binary':\n",
        "            confidence = float(prediction[0][0])\n",
        "            predicted_class = config['classes'][1] if confidence > 0.5 else config['classes'][0]\n",
        "            confidence = confidence if confidence > 0.5 else 1 - confidence\n",
        "        else:\n",
        "            class_idx = np.argmax(prediction[0])\n",
        "            predicted_class = config['classes'][class_idx]\n",
        "            confidence = float(prediction[0][class_idx])\n",
        "\n",
        "        return {\n",
        "            'disease': disease_name,\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'all_probabilities': prediction[0].tolist()\n",
        "        }\n",
        "\n",
        "    def predict_multiple_diseases(self, img_path):\n",
        "        \"\"\"Predict multiple diseases for single image\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for disease_name in self.models.keys():\n",
        "            result = self.predict_image(img_path, disease_name)\n",
        "            if result:\n",
        "                results[disease_name] = result\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "sPndKMyGzNrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"Multi-Disease Prediction System\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = DiseaseTrainer()\n",
        "\n",
        "    # Select diseases to train (modify as needed)\n",
        "    diseases_to_train = ['pneumonia', 'skin_cancer', 'brain_tumor']\n",
        "\n",
        "    try:\n",
        "        # Train all models with robust error handling\n",
        "        successful_trainings, failed_trainings = trainer.train_all_diseases(diseases_to_train, epochs=15)\n",
        "\n",
        "        if successful_trainings:\n",
        "            # Visualize training results for successful trainings only\n",
        "            if trainer.histories:\n",
        "                vis_tools = VisualizationTools()\n",
        "                vis_tools.plot_training_history(trainer.histories)\n",
        "\n",
        "            # Initialize predictor and load models\n",
        "            predictor = MultiDiseasePredictor()\n",
        "            predictor.load_models(successful_trainings)\n",
        "\n",
        "            print(f\"\\n🎉 System ready! Successfully trained {len(successful_trainings)} models.\")\n",
        "\n",
        "            # Provide usage instructions\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"USAGE INSTRUCTIONS\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"To make predictions on new images:\")\n",
        "            print(\"1. Single disease prediction:\")\n",
        "            print(\"   result = predictor.predict_image('image_path.jpg', 'pneumonia')\")\n",
        "            print(\"2. Multi-disease prediction:\")\n",
        "            print(\"   results = predictor.predict_multiple_diseases('image_path.jpg')\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n❌ No models were successfully trained. Please check the errors above.\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error during training pipeline: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "    return trainer, predictor"
      ],
      "metadata": {
        "id": "skJW7nS5zT1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Example usage:\n",
        "\n",
        "# 1. Train models for specific diseases\n",
        "trainer = DiseaseTrainer()\n",
        "trainer.train_all_diseases(['pneumonia', 'skin_cancer'], epochs=10)\n",
        "\n",
        "# 2. Load trained models and make predictions\n",
        "predictor = MultiDiseasePredictor()\n",
        "predictor.load_models(['pneumonia', 'skin_cancer'])\n",
        "\n",
        "# 3. Predict single disease\n",
        "result = predictor.predict_image('test_image.jpg', 'pneumonia')\n",
        "print(f\"Prediction: {result['predicted_class']} (Confidence: {result['confidence']:.2f})\")\n",
        "\n",
        "# 4. Predict multiple diseases\n",
        "results = predictor.predict_multiple_diseases('test_image.jpg')\n",
        "for disease, result in results.items():\n",
        "    print(f\"{disease}: {result['predicted_class']} ({result['confidence']:.2f})\")\n",
        "\n",
        "# 5. Visualize Grad-CAM\n",
        "gradcam = GradCAMVisualizer()\n",
        "gradcam.display_gradcam('test_image.jpg', predictor.models['pneumonia'], (224, 224))\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer, predictor = main()"
      ],
      "metadata": {
        "id": "Qzcd_JgazXZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}